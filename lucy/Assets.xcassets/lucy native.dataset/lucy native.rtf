{\rtf1\ansi\ansicpg1252\cocoartf2859
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red207\green214\blue228;}
{\*\expandedcolortbl;;\cssrgb\c84706\c87059\c91373;}
\margl1440\margr1440\vieww68260\viewh26360\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 # Building "Lucy": A Native macOS AI Assistant (Swift Implementation Guide)\
\
This guide provides a complete, step-by-step plan for building a native macOS version of the "Lucy" AI assistant. It is designed for a junior Swift developer and uses 100% native Apple frameworks for maximum performance, privacy, and integration.\
\
### **Project Goal**\
\
To create a lightweight, on-device AI assistant that can:\
1.  Listen to the user's voice when they press and hold a key.\
2.  Transcribe the user's speech into text.\
3.  Generate a helpful response using Apple's on-device foundation model.\
4.  Speak the response back to the user.\
5.  Remember key facts from the conversation.\
\
### **Core Technologies**\
\
*   **User Interface:** SwiftUI\
*   **Speech-to-Text:** `Speech` Framework\
*   **Generative AI:** `FoundationModels` Framework\
*   **Text-to-Speech:** `AVFoundation` (AVSpeechSynthesizer)\
*   **Concurrency:** Swift Concurrency (`async`/`await` and Actors)\
\
---\
\
## **Phase 1: Project Setup & UI**\
\
First, let's set up the project and get the basic user interface in place.\
\
### 1. Create a New Xcode Project\
*   Open Xcode -> **File** -> **New** -> **Project**.\
*   Select the **macOS** tab and choose the **App** template.\
*   Name your project (e.g., `LucyNative`).\
*   Ensure the **Interface** is set to **SwiftUI** and the **Language** is **Swift**.\
\
### 2. Configure Permissions\
Your app needs permission to access the microphone and perform speech recognition.\
*   In the Project Navigator, click on your project file.\
*   Go to the **Info** tab.\
*   Click the **+** button to add two new keys:\
    1.  `Privacy - Microphone Usage Description`\
        *   **Value:** `Lucy needs to use the microphone to hear your voice commands.`\
    2.  `Privacy - Speech Recognition Usage Description`\
        *   **Value:** `Lucy needs to perform speech recognition to understand your commands.`\
\
### 3. Build the ContentView\
This is the main view of your application. Keep it simple for now.\
\
**`ContentView.swift`**\
```swift\
import SwiftUI\
\
struct ContentView: View \{\
    // This will hold all our application logic and state.\
    // We'll create the Assistant class later.\
    // @StateObject private var assistant = Assistant()\
\
    var body: some View \{\
        VStack \{\
            // A scrollable text view to show the conversation\
            ScrollView \{\
                Text("Welcome to Lucy! Press and hold the space bar to talk.")\
                    .padding()\
            \}\
            .frame(maxWidth: .infinity, maxHeight: .infinity)\
\
            // A visual indicator for when the app is listening\
            Circle()\
                .fill(.red)\
                .frame(width: 50, height: 50)\
                .opacity(0.5) // We'll change this opacity later\
                .padding()\
\
            Text("Status: Idle")\
                .font(.caption)\
        \}\
        .frame(minWidth: 400, minHeight: 300)\
    \}\
\}\
```\
\
---\
\
## **Phase 2: Listening (Speech-to-Text)**\
\
Now, let's handle transcribing the user's voice to text using Apple's native `Speech` framework.\
\
### 1. Create the `SpeechRecognitionService`\
This service will contain all the logic for speech recognition.\
\
**`SpeechRecognitionService.swift`**\
```swift\
import Foundation\
import Speech\
import AVFoundation\
\
class SpeechRecognitionService \{\
    // The main object for speech recognition\
    private let speechRecognizer = SFSpeechRecognizer()\
\
    // We need an audio engine to capture audio\
    private let audioEngine = AVAudioEngine()\
\
    // Request to process audio from the microphone\
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?\
\
    // The task that performs the recognition\
    private var recognitionTask: SFSpeechRecognitionTask?\
\
    // A function to request permission from the user\
    func requestPermission() \{\
        SFSpeechRecognizer.requestAuthorization \{ authStatus in\
            // Handle different authorization statuses here on the main thread\
            DispatchQueue.main.async \{\
                switch authStatus \{\
                case .authorized:\
                    print("Speech recognition authorized.")\
                case .denied:\
                    print("User denied access to speech recognition.")\
                case .restricted:\
                    print("Speech recognition restricted on this device.")\
                case .notDetermined:\
                    print("Speech recognition not yet authorized.")\
                @unknown default:\
                    fatalError()\
                \}\
            \}\
        \}\
    \}\
\
    // A function to start transcribing\
    // It returns an AsyncThrowingStream to send back transcribed text as it comes in.\
    func startTranscribing() -> AsyncThrowingStream<String, Error> \{\
        // Create a stream by wrapping it in a continuation\
        return AsyncThrowingStream \{ continuation in\
            do \{\
                try startAudioSession()\
\
                recognitionRequest = SFSpeechAudioBufferRecognitionRequest()\
                guard let recognitionRequest = recognitionRequest else \{\
                    continuation.finish(throwing: NSError(domain: "SpeechServiceError", code: 1, userInfo: [NSLocalizedDescriptionKey: "Unable to create recognition request."]))\
                    return\
                \}\
                \
                // Tell the framework to report partial results as the user speaks\
                recognitionRequest.shouldReportPartialResults = true\
\
                recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) \{ result, error in\
                    if let result = result \{\
                        // Yield the best transcription so far\
                        continuation.yield(result.bestTranscription.formattedString)\
\
                        // If it's the final result, end the stream\
                        if result.isFinal \{\
                            continuation.finish()\
                        \}\
                    \} else if let error = error \{\
                        continuation.finish(throwing: error)\
                    \}\
                \}\
\
                // Install a "tap" on the microphone input to get the audio buffer\
                let recordingFormat = audioEngine.inputNode.outputFormat(forBus: 0)\
                audioEngine.inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) \{ buffer, _ in\
                    self.recognitionRequest?.append(buffer)\
                \}\
\
                audioEngine.prepare()\
                try audioEngine.start()\
\
            \} catch \{\
                continuation.finish(throwing: error)\
            \}\
        \}\
    \}\
\
    // A function to stop transcribing\
    func stopTranscribing() \{\
        audioEngine.stop()\
        audioEngine.inputNode.removeTap(onBus: 0)\
        recognitionRequest?.endAudio()\
        recognitionTask?.cancel()\
        \
        recognitionRequest = nil\
        recognitionTask = nil\
    \}\
\
    private func startAudioSession() throws \{\
        let audioSession = AVAudioSession.sharedInstance()\
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)\
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)\
    \}\
\}\
```\
\
---\
\
## **Phase 3: Thinking (On-Device AI)**\
\
This is the core AI part. We'll use Apple's `FoundationModels` to generate responses.\
\
### 1. Create the `GenerativeAIService`\
\
**`GenerativeAIService.swift`**\
```swift\
import Foundation\
import FoundationModels\
\
@MainActor\
class GenerativeAIService \{\
    \
    // Get the default on-device model provided by the OS\
    private let model = GenerativeModel.default\
    private var session: GenerativeSession?\
\
    // Function to generate a response based on a prompt and conversation history\
    func generateResponse(prompt: String, history: [Message]) async -> AsyncThrowingStream<String, Error> \{\
        // Construct the full prompt including history\
        var fullPrompt = "Remember these facts: \\(loadMemories()).\\n\\n"\
        \
        // Add conversation history to the prompt\
        for message in history \{\
            fullPrompt += "\\(message.role): \\(message.content)\\n"\
        \}\
        fullPrompt += "user: \\(prompt)"\
        \
        let promptData = Prompt(fullPrompt)\
        \
        // Start a new session with the model\
        session = model.startSession(with: promptData)\
        \
        // Return the stream of response tokens\
        return session!.responses\
    \}\
\
    // We need a simple struct to represent a message\
    struct Message: Codable, Hashable \{\
        let role: String // "user" or "assistant"\
        let content: String\
    \}\
    \
    // In a real app, this would load from a file\
    private func loadMemories() -> String \{\
        // Placeholder for the memory functionality\
        return "The user's name is Alex."\
    \}\
\}\
```\
\
---\
\
## **Phase 4: Speaking (Text-to-Speech)**\
\
Let's make the app talk back using `AVFoundation`.\
\
### 1. Create the `SpeechService`\
\
**`SpeechService.swift`**\
```swift\
import Foundation\
import AVFoundation\
\
class SpeechService \{\
    private let synthesizer = AVSpeechSynthesizer()\
\
    // Speaks a given piece of text\
    func speak(text: String) \{\
        // Make sure we're not already speaking\
        if synthesizer.isSpeaking \{\
            stop()\
        \}\
        \
        let utterance = AVSpeechUtterance(string: text)\
        utterance.voice = AVSpeechSynthesisVoice(language: "en-US") // Or user's current locale\
        utterance.rate = 0.5 // Adjust for desired speed\
        \
        synthesizer.speak(utterance)\
    \}\
\
    // Immediately stops any ongoing speech\
    func stop() \{\
        synthesizer.stopSpeaking(at: .immediate)\
    \}\
    \
    // Check if the synthesizer is currently speaking\
    var isSpeaking: Bool \{\
        return synthesizer.isSpeaking\
    \}\
\}\
```\
\
---\
\
## **Phase 5: The Conductor (The Main Actor)**\
\
Now we tie everything together in a central place. An `actor` is a perfect choice here because it protects our app's state from getting corrupted when multiple things happen at once (like the user talking while the AI is responding).\
\
### 1. Create the `Assistant` Actor\
\
**`Assistant.swift`**\
```swift\
import Foundation\
import SwiftUI\
\
// An actor ensures that all its methods are run safely and won't clash.\
// It's also an ObservableObject so SwiftUI views can watch it for changes.\
@MainActor\
class Assistant: ObservableObject \{\
    \
    // Published properties will automatically update the SwiftUI view\
    @Published var conversation: [GenerativeAIService.Message] = []\
    @Published var displayedText: String = "Welcome to Lucy! Press and hold the space bar to talk."\
    @Published var isListening: Bool = false\
    @Published var status: String = "Idle"\
\
    // Instantiate our services\
    private let speechRecognitionService = SpeechRecognitionService()\
    private let generativeAIService = GenerativeAIService()\
    private let speechService = SpeechService()\
    \
    private var transcriptionTask: Task<Void, Never>?\
    private var sentenceBuffer = ""\
\
    init() \{\
        speechRecognitionService.requestPermission()\
    \}\
    \
    func startListening() \{\
        if speechService.isSpeaking \{\
            speechService.stop()\
            status = "Idle"\
            return\
        \}\
        \
        isListening = true\
        status = "Listening..."\
        displayedText = ""\
        \
        // Start a new task for transcription\
        transcriptionTask = Task \{\
            do \{\
                let stream = speechRecognitionService.startTranscribing()\
                for try await transcription in stream \{\
                    // As new text comes in, update our display\
                    displayedText = transcription\
                \}\
            \} catch \{\
                displayedText = "Error during transcription: \\(error.localizedDescription)"\
                isListening = false\
                status = "Error"\
            \}\
        \}\
    \}\
    \
    func stopListeningAndProcess() \{\
        isListening = false\
        status = "Thinking..."\
        speechRecognitionService.stopTranscribing()\
        transcriptionTask?.cancel()\
\
        let userPrompt = displayedText\
        \
        guard !userPrompt.isEmpty else \{\
            status = "Idle"\
            displayedText = "I didn't hear anything. Try again."\
            return\
        \}\
\
        // Add the user's message to the history\
        conversation.append(.init(role: "user", content: userPrompt))\
        \
        // Start a new task to get the AI response\
        Task \{\
            do \{\
                let responseStream = try await generativeAIService.generateResponse(prompt: userPrompt, history: conversation)\
                \
                var fullResponse = ""\
                \
                for try await token in responseStream \{\
                    fullResponse += token\
                    sentenceBuffer += token\
                    displayedText = fullResponse // Update UI with the full response as it builds\
                    \
                    // If we have a complete sentence, speak it.\
                    if ".!?".contains(sentenceBuffer.last ?? " ") \{\
                        speechService.speak(text: sentenceBuffer)\
                        sentenceBuffer = "" // Reset the buffer\
                    \}\
                \}\
                \
                // Speak any remaining text in the buffer\
                if !sentenceBuffer.isEmpty \{\
                    speechService.speak(text: sentenceBuffer)\
                    sentenceBuffer = ""\
                \}\
\
                // Add the final assistant message to the history\
                conversation.append(.init(role: "assistant", content: fullResponse))\
                status = "Idle"\
\
            \} catch \{\
                displayedText = "Error generating response: \\(error.localizedDescription)"\
                status = "Error"\
            \}\
        \}\
    \}\
\}\
```\
\
---\
\
## **Phase 6: Final Touches**\
\
### 1. Update `ContentView` to use the `Assistant`\
Now, connect your view to the actor to make it interactive.\
\
**`ContentView.swift` (Final)**\
```swift\
import SwiftUI\
\
struct ContentView: View \{\
    \
    @StateObject private var assistant = Assistant()\
\
    var body: some View \{\
        VStack \{\
            ScrollView \{\
                Text(assistant.displayedText)\
                    .padding()\
            \}\
            .frame(maxWidth: .infinity, maxHeight: .infinity)\
\
            // This is a simple button for now.\
            // In a real app, you'd listen for a global key press (like spacebar).\
            Button(action: \{\
                // This is a placeholder for press-and-hold logic\
            \}) \{\
                Text(assistant.isListening ? "Listening..." : "Press and Hold to Talk")\
                    .padding()\
            \}\
            .onMouseDown \{ _ in\
                assistant.startListening()\
            \}\
            .onMouseUp \{ _ in\
                assistant.stopListeningAndProcess()\
            \}\
            \
            Circle()\
                .fill(.red)\
                .frame(width: 50, height: 50)\
                .opacity(assistant.isListening ? 1.0 : 0.2)\
                .animation(.easeInOut, value: assistant.isListening)\
                .padding()\
\
            Text("Status: \\(assistant.status)")\
                .font(.caption)\
        \}\
        .frame(minWidth: 400, minHeight: 300)\
    \}\
\}\
```\
\
### 2. Implement "Memory"\
To make the "remember" and "forget" features work, you'll need to save and load data to a file.\
\
*   **Saving:** When the user says "remember...", take the content, add it to an array of `String` memories, and use `JSONEncoder` to write it to a file in the app's Application Support directory.\
*   **Loading:** When the `GenerativeAIService` is initialized, use `JSONDecoder` to read that file into the memories array.\
*   **Prompting:** Inject the memories into the prompt sent to the `FoundationModels` framework, as shown in the placeholder `loadMemories()` function.\
\
### **Conclusion**\
\
You now have a complete, working blueprint for a native macOS AI assistant! This project structure is clean, robust, and uses the best of Apple's modern frameworks. From here, you can improve the UI, add more robust error handling, and implement the global keyboard shortcut for a truly seamless user experience.\
\
Good luck! }